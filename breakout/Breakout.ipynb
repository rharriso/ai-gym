{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rharriso/miniconda2/envs/sonic/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/rharriso/Desktop/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from progressbar import ProgressBar\n",
    "from time import sleep\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "from matplotlib import image\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from utils import pre_process_frame\n",
    "\n",
    "# init env\n",
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(action_size, alpha, initializer='glorot_uniform'):\n",
    "    optimizer = optimizers.RMSprop(lr=alpha, clipvalue=0.5)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Conv2D(\n",
    "            32,\n",
    "            (8, 8),\n",
    "            strides=(4,4),\n",
    "            input_shape=(84, 84, 4),\n",
    "            activation='relu',\n",
    "            kernel_initializer=initializer,\n",
    "            data_format=\"channels_last\"\n",
    "        ),\n",
    "        Conv2D(\n",
    "            64,\n",
    "            (4,4),\n",
    "            strides=(2,2),\n",
    "            activation='relu',\n",
    "            kernel_initializer=initializer\n",
    "        ),\n",
    "        Conv2D(\n",
    "            64,\n",
    "            (3,3),\n",
    "            strides=(1,1),\n",
    "            activation='relu',\n",
    "            kernel_initializer=initializer\n",
    "        ),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_initializer=initializer),\n",
    "        Dense(action_size, activation='linear', kernel_initializer=initializer)\n",
    "    ])\n",
    "    net.compile(loss='mse', optimizer=optimizer)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.00025\n",
    "gamma=0.99\n",
    "training_frame_count = 5e7 # 50 million frames\n",
    "greedy_end = 1e6 # 1 million exploration frames\n",
    "action_update_interval = 4\n",
    "q_update_interval = 16\n",
    "qhat_update_interval = 500\n",
    "sample_size=32 # how many samples do we take each time\n",
    "min_replay_size = 500 # whhen does training start (replay mem size)\n",
    "max_replay_size = 25000 # what is the max replay memory\n",
    "chart_interval = 1e4 # every 10k frames show me a chart\n",
    "\n",
    "# step increment at begining of loop\n",
    "step = 0\n",
    "\n",
    "# trainy boi\n",
    "training_started = False\n",
    "frame_memory = np.zeros((max_replay_size, 84, 84, 4), dtype='float')\n",
    "prime_frame_memory = np.zeros((max_replay_size, 84, 84, 4), dtype='float')\n",
    "action_memory = np.zeros((max_replay_size, 1), dtype='int')\n",
    "reward_memory = np.zeros((max_replay_size, 1), dtype='float')\n",
    "done_memory = np.zeros((max_replay_size, 1), dtype='bool')\n",
    "\n",
    "replay_index = -1\n",
    "\n",
    "# stats\n",
    "training_rewards = []\n",
    "rewards = []\n",
    "mean_rewards = []\n",
    "mean_training_rewards = []\n",
    "epsilons = []\n",
    "training_loss = []\n",
    "training_step = []\n",
    "training_score = []\n",
    "test_score = []\n",
    "last_episode = 0\n",
    "\n",
    "# init networks\n",
    "q_net = create_network(action_size, alpha)\n",
    "q_hat_net = create_network(action_size, alpha, 'zeros')\n",
    "\n",
    "pbar = ProgressBar()\n",
    "\n",
    "\n",
    "# SVG(model_to_dot(q_net).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1 of 50000000.0) |        | Elapsed Time: 0:00:00 ETA:  70 days, 19:41:39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Episode: epsilon %s 1.0000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000000.0 of 50000000.0) |########| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epsilon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-436f2efb39e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mtraining_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mmean_training_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mepsilons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# display progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilon' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def time_usage(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        beg_ts = time.time()\n",
    "        retval = func(*args, **kwargs)\n",
    "        end_ts = time.time()\n",
    "        # print(\"elapsed time %s: %f\" % (func.__name__, end_ts - beg_ts))\n",
    "        return retval\n",
    "    return wrapper\n",
    "\n",
    "def run_episode(frame_index, render=True):\n",
    "    if frame_index < greedy_end:\n",
    "        epsilon = 1. - (frame_index / greedy_end) * 0.9\n",
    "        \n",
    "    else:\n",
    "        epsilon = 0.1\n",
    "    print(\"Start Episode: epsilon %s\", epsilon)\n",
    "    \n",
    "    moves = 0\n",
    "    beg_ts = time.time()\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    old_lives = -1\n",
    "    prev_frame = np.zeros_like(frame) # the \"prevvious\" frame is nothing\n",
    "    phi_frame = pre_process_frame(frame, prev_frame)\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            phi_shape = phi_frame.shape\n",
    "            shaped_frame = phi_frame.reshape(1, phi_shape[0], phi_shape[1], phi_shape[2])\n",
    "            action = np.argmax(q_net.predict(shaped_frame))\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        for i in range(action_update_interval):\n",
    "            prime_frame, _reward, done, ale_lives = env.step(action)\n",
    "            reward += _reward\n",
    "            lives = ale_lives['ale.lives']\n",
    "            \n",
    "            if lives < old_lives:\n",
    "                reward -= 10\n",
    "            old_lives = lives\n",
    "            \n",
    "            done = done or lives == 0\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        phi_prime_frame = pre_process_frame(frame, prime_frame)\n",
    "        \n",
    "        yield phi_frame, action, reward, phi_prime_frame, done, lives\n",
    "\n",
    "        phi_frame = phi_prime_frame\n",
    "        prev_frame = frame\n",
    "        frame = prime_frame\n",
    "        moves += 1\n",
    "\n",
    "    end_ts = time.time()\n",
    "    # print(\"elapsed time run_episode: %s moves %fs\" % (moves, end_ts - beg_ts))\n",
    "\n",
    "weight_updates = 0\n",
    "\n",
    "# @time_usage\n",
    "def update_weights():\n",
    "    # crop memory\n",
    "    global frame_memory, action_memory, reward_memory, prime_frame_memory, done_memory, weight_updates, replay_index\n",
    "\n",
    "    memory_sample = random.sample(\n",
    "        range(0, min(replay_index, max_replay_size)),\n",
    "        sample_size\n",
    "    )\n",
    "\n",
    "    _frame_batch = frame_memory[memory_sample]\n",
    "    _prime_frame_batch = prime_frame_memory[memory_sample]\n",
    "    _action_batch = action_memory[memory_sample]\n",
    "    _reward_batch = reward_memory[memory_sample]\n",
    "    _done_batch = done_memory[memory_sample]\n",
    "\n",
    "    _predictions = q_net.predict(_frame_batch)\n",
    "    _corrections = _predictions.copy()\n",
    "    _prime_predictions = q_hat_net.predict(_prime_frame_batch)\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        _done = _done_batch[i]\n",
    "        _reward = _reward_batch[i]\n",
    "        _action = _action_batch[i]\n",
    "\n",
    "        _prime_prediction = _prime_predictions[i]\n",
    "        if _done:\n",
    "            _corrections[i, _action] = _reward_batch[i]\n",
    "        else:\n",
    "            _corrections[i, _action] = _reward + gamma * np.max(_prime_prediction)\n",
    "    loss = q_net.train_on_batch(_frame_batch, _corrections)\n",
    "    \n",
    "    if weight_updates % qhat_update_interval == 0:\n",
    "        # print(\"updating target\")\n",
    "        q_hat_net.set_weights(q_net.get_weights())\n",
    "    weight_updates += 1\n",
    "    return loss\n",
    "\n",
    "step = -1\n",
    "training_started = False\n",
    "with ProgressBar(max_value=training_frame_count) as bar:\n",
    "    bar.update(0)\n",
    "    while step < training_frame_count:\n",
    "        training_score = 0\n",
    "        # for every action in the episode\n",
    "        for (\n",
    "            frame,\n",
    "            action,\n",
    "            reward,\n",
    "            prime_frame,\n",
    "            done,\n",
    "            lives\n",
    "        ) in run_episode(step, render=True):\n",
    "            step += 1\n",
    "            bar.update(step)\n",
    "            #\n",
    "            replay_index += 1\n",
    "            replay_array_index = replay_index % max_replay_size\n",
    "            frame_memory[replay_array_index] = frame\n",
    "            prime_frame_memory[replay_array_index] = prime_frame\n",
    "            action_memory[replay_array_index] = action\n",
    "            reward_memory[replay_array_index] = reward\n",
    "            done_memory[replay_array_index] = done\n",
    "\n",
    "            training_score += reward\n",
    "\n",
    "            # if time do the training\n",
    "            if step % q_update_interval == 0 and min_replay_size < replay_index:\n",
    "                if not training_started:\n",
    "                    training_started = True\n",
    "                    # print(\"Training Started: \", step)\n",
    "                loss = update_weights()\n",
    "                training_loss.append(loss)\n",
    "                training_step.append(step)\n",
    "\n",
    "\n",
    "        # print(\"Record metrics\")\n",
    "        training_rewards.append(training_score)\n",
    "        mean_training_rewards.append(np.mean(training_rewards[-100:]))\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "        # display progress\n",
    "        if (step + 1) % chart_interval == 0:\n",
    "            index_step = 10\n",
    "            episodes_num = np.arange(0, len(mean_training_rewards))[0::index_step]\n",
    "            _mean_training_rewards = mean_training_rewards[0::index_step]\n",
    "            #         _mean_rewards = mean_rewards[0::index_step]\n",
    "            #         _rewards = rewards[0::index_step]\n",
    "            _training_rewards = training_rewards[0::index_step]\n",
    "            _epsilons = epsilons[0::index_step]\n",
    "            episode_index = len(mean_training_rewards)\n",
    "\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 10), dpi=100)\n",
    "            plt.subplot(211)\n",
    "            plt.plot(episodes_num, _training_rewards, marker='o', markersize=1, label='Training Reward')\n",
    "            #         par1.plot(episodes_num, _rewards, marker='o', markersize=5, label='Test Reward')\n",
    "            plt.plot(episodes_num, _mean_training_rewards, marker='x', markersize=1, label='Mean Training Rewards')\n",
    "            #         par1.plot(episodes_num, _mean_rewards, marker='o', markersize=5, label='Mean Test Reward')\n",
    "            plt.legend(loc='lower left')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('Mean Reward')\n",
    "\n",
    "            plt.subplot(212)\n",
    "            plt.plot(training_step[0::index_step], training_loss[0::index_step], marker='o', markersize=1, label='Training Loss', color='purple')\n",
    "            plt.ylim(-0.1, 10)\n",
    "\n",
    "            plt.pause(0.01)\n",
    "\n",
    "            # save after charting\n",
    "            model_file = '%s-%s-%s-pool-actions.weights' % (env_name, alpha, episode)\n",
    "            q_net.save_weights(model_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87ba362ace5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s-%s-%s.weights'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_name' is not defined"
     ]
    }
   ],
   "source": [
    "model_file = '%s-%s-%s.weights' % (env_name, alpha, episode)\n",
    "net.save_weights(model_file)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
