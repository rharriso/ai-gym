{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from progressbar import ProgressBar\n",
    "from time import sleep\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "from matplotlib import image\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from utils import pre_process_frame\n",
    "\n",
    "# init env\n",
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(action_size, alpha, initializer='glorot_uniform'):\n",
    "    optimizer = optimizers.RMSprop(lr=alpha, clipvalue=0.5)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Conv2D(\n",
    "            32,\n",
    "            (8, 8),\n",
    "            strides=(4,4),\n",
    "            input_shape=(84, 84, 4),\n",
    "            activation='relu',\n",
    "            kernel_initializer=initializer,\n",
    "            data_format=\"channels_last\"\n",
    "        ),\n",
    "        Conv2D(\n",
    "            64,\n",
    "            (4,4),\n",
    "            strides=(2,2),\n",
    "            activation='relu',\n",
    "            kernel_initializer=initializer\n",
    "        ),\n",
    "        Conv2D(\n",
    "            64,\n",
    "            (3,3),\n",
    "            strides=(1,1),\n",
    "            activation='relu',\n",
    "            kernel_initializer=initializer\n",
    "        ),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_initializer=initializer),\n",
    "        Dense(action_size, activation='linear', kernel_initializer=initializer)\n",
    "    ])\n",
    "    net.compile(loss='mse', optimizer=optimizer)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.00025\n",
    "gamma=0.99\n",
    "training_frame_count = 5e7 # 50 million frames\n",
    "greedy_end = 1e6 # 1 million exploration frames\n",
    "action_update_interval = 4\n",
    "q_update_interval = 16\n",
    "qhat_update_interval = 500\n",
    "sample_size=32 # how many samples do we take each time\n",
    "min_replay_size = 500 # whhen does training start (replay mem size)\n",
    "max_replay_size = 25000 # what is the max replay memory\n",
    "chart_interval = 1e3 # every 10k frames show me a chart\n",
    "\n",
    "# step increment at begining of loop\n",
    "step = 0\n",
    "\n",
    "# trainy boi\n",
    "training_started = False\n",
    "frame_memory = np.zeros((max_replay_size, 84, 84, 4), dtype='float')\n",
    "prime_frame_memory = np.zeros((max_replay_size, 84, 84, 4), dtype='float')\n",
    "action_memory = np.zeros((max_replay_size, 1), dtype='int')\n",
    "reward_memory = np.zeros((max_replay_size, 1), dtype='float')\n",
    "done_memory = np.zeros((max_replay_size, 1), dtype='bool')\n",
    "\n",
    "replay_index = -1\n",
    "\n",
    "# stats\n",
    "try:\n",
    "    os.remove(\"training_loss.csv\")\n",
    "    os.remove(\"training_reward.csv\")    \n",
    "except OSError:\n",
    "    pass\n",
    "loss_dump = open(\"training_loss.csv\", \"a\")\n",
    "reward_dump = open(\"training_reward.csv\", \"a\")\n",
    "\n",
    "last_episode = 0\n",
    "\n",
    "# init networks\n",
    "q_net = create_network(action_size, alpha)\n",
    "q_hat_net = create_network(action_size, alpha, 'zeros')\n",
    "\n",
    "pbar = ProgressBar()\n",
    "\n",
    "\n",
    "# SVG(model_to_dot(q_net).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50000000.0 of 50000000.0) |#########| Elapsed Time: 0:00:05 Time: 0:00:05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e5b6ce5f2c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mlives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         ) in run_episode(step, render=True):\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e5b6ce5f2c35>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(frame_index, render)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mphi_prime_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reinforcement-learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reinforcement-learning/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reinforcement-learning/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/reinforcement-learning/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def time_usage(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        beg_ts = time.time()\n",
    "        retval = func(*args, **kwargs)\n",
    "        end_ts = time.time()\n",
    "        # print(\"elapsed time %s: %f\" % (func.__name__, end_ts - beg_ts))\n",
    "        return retval\n",
    "    return wrapper\n",
    "\n",
    "def run_episode(frame_index, render=True):\n",
    "    if frame_index < greedy_end:\n",
    "        epsilon = 1. - (frame_index / greedy_end) * 0.9\n",
    "    else:\n",
    "        epsilon = 0.1\n",
    "    \n",
    "    moves = 0\n",
    "    beg_ts = time.time()\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    old_lives = -1\n",
    "    prev_frame = np.zeros_like(frame) # the \"prevvious\" frame is nothing\n",
    "    phi_frame = pre_process_frame(frame, prev_frame)\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            phi_shape = phi_frame.shape\n",
    "            shaped_frame = phi_frame.reshape(1, phi_shape[0], phi_shape[1], phi_shape[2])\n",
    "            action = np.argmax(q_net.predict(shaped_frame))\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        for i in range(action_update_interval):\n",
    "            prime_frame, _reward, done, ale_lives = env.step(action)\n",
    "            reward += _reward\n",
    "            lives = ale_lives['ale.lives']\n",
    "            \n",
    "            if lives < old_lives:\n",
    "                reward -= 10\n",
    "            old_lives = lives\n",
    "            \n",
    "            done = done or lives == 0\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        phi_prime_frame = pre_process_frame(frame, prime_frame)\n",
    "\n",
    "        yield phi_frame, action, reward, phi_prime_frame, done, lives\n",
    "\n",
    "        phi_frame = phi_prime_frame\n",
    "        prev_frame = frame\n",
    "        frame = prime_frame\n",
    "        moves += 1\n",
    "\n",
    "\n",
    "weight_updates = 0\n",
    "\n",
    "# @time_usage\n",
    "def update_weights():\n",
    "    # crop memory\n",
    "    global frame_memory, action_memory, reward_memory, prime_frame_memory, done_memory, weight_updates, replay_index\n",
    "\n",
    "    memory_sample = random.sample(\n",
    "        range(0, min(replay_index, max_replay_size)),\n",
    "        sample_size\n",
    "    )\n",
    "\n",
    "    _frame_batch = frame_memory[memory_sample]\n",
    "    _prime_frame_batch = prime_frame_memory[memory_sample]\n",
    "    _action_batch = action_memory[memory_sample]\n",
    "    _reward_batch = reward_memory[memory_sample]\n",
    "    _done_batch = done_memory[memory_sample]\n",
    "\n",
    "    _predictions = q_net.predict(_frame_batch)\n",
    "    _corrections = _predictions.copy()\n",
    "    _prime_predictions = q_hat_net.predict(_prime_frame_batch)\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        _done = _done_batch[i]\n",
    "        _reward = _reward_batch[i]\n",
    "        _action = _action_batch[i]\n",
    "\n",
    "        _prime_prediction = _prime_predictions[i]\n",
    "        if _done:\n",
    "            _corrections[i, _action] = _reward_batch[i]\n",
    "        else:\n",
    "            _corrections[i, _action] = _reward + gamma * np.max(_prime_prediction)\n",
    "    loss = q_net.train_on_batch(_frame_batch, _corrections)\n",
    "    \n",
    "    if weight_updates % qhat_update_interval == 0:\n",
    "        q_hat_net.set_weights(q_net.get_weights())\n",
    "    weight_updates += 1\n",
    "    return loss\n",
    "\n",
    "step = -1\n",
    "training_started = False\n",
    "with ProgressBar(max_value=training_frame_count) as bar:\n",
    "    bar.update(0)\n",
    "    while step < training_frame_count:\n",
    "        training_score = 0\n",
    "        # for every action in the episode\n",
    "        for (\n",
    "            frame,\n",
    "            action,\n",
    "            reward,\n",
    "            prime_frame,\n",
    "            done,\n",
    "            lives\n",
    "        ) in run_episode(step, render=True):\n",
    "            step += 1\n",
    "            bar.update(step)\n",
    "            #\n",
    "            replay_index += 1\n",
    "            replay_array_index = replay_index % max_replay_size\n",
    "            frame_memory[replay_array_index] = frame\n",
    "            prime_frame_memory[replay_array_index] = prime_frame\n",
    "            action_memory[replay_array_index] = action\n",
    "            reward_memory[replay_array_index] = reward\n",
    "            done_memory[replay_array_index] = done\n",
    "\n",
    "            training_score += reward\n",
    "\n",
    "            # if time do the training\n",
    "            if step % q_update_interval == 0 and min_replay_size < replay_index:\n",
    "                if not training_started:\n",
    "                    training_started = True\n",
    "                    # print(\"Training Started: \", step)\n",
    "                loss = update_weights()\n",
    "                loss_dump.write(\"%s, %s\\n\" % (loss, step))\n",
    "\n",
    "            reward_dump.write(\"%s, %s\\n\" % (training_score, step))\n",
    "\n",
    "\n",
    "            # display progress\n",
    "            if step % chart_interval == 0:\n",
    "#                 index_step = 10\n",
    "#                 episodes_num = np.arange(0, len(mean_training_rewards))[0::index_step]\n",
    "#                 _mean_training_rewards = mean_training_rewards[0::index_step]\n",
    "#                 #         _mean_rewards = mean_rewards[0::index_step]\n",
    "#                 #         _rewards = rewards[0::index_step]\n",
    "#                 _training_rewards = training_rewards[0::index_step]\n",
    "#                 _epsilons = epsilons[0::index_step]\n",
    "#                 episode_index = len(mean_training_rewards)\n",
    "\n",
    "\n",
    "#                 fig = plt.figure(figsize=(8, 10), dpi=100)\n",
    "#                 plt.subplot(211)\n",
    "#                 plt.plot(episodes_num, _training_rewards, marker='o', markersize=1, label='Training Reward')\n",
    "#                 #         par1.plot(episodes_num, _rewards, marker='o', markersize=5, label='Test Reward')\n",
    "#                 plt.plot(episodes_num, _mean_training_rewards, marker='x', markersize=1, label='Mean Training Rewards')\n",
    "#                 #         par1.plot(episodes_num, _mean_rewards, marker='o', markersize=5, label='Mean Test Reward')\n",
    "#                 plt.legend(loc='lower left')\n",
    "#                 plt.xlabel('epoch')\n",
    "#                 plt.ylabel('Mean Reward')\n",
    "\n",
    "#                 plt.subplot(212)\n",
    "#                 plt.plot(training_step[0::index_step], training_loss[0::index_step], marker='o', markersize=1, label='Training Loss', color='purple')\n",
    "#                 plt.ylim(-0.1, 10)\n",
    "\n",
    "#                 plt.pause(0.01)\n",
    "\n",
    "                # save after charting\n",
    "                model_file = '%s-%s-%s-pool-actions.weights' % (env_name, alpha, step)\n",
    "                q_net.save_weights(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87ba362ace5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s-%s-%s.weights'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_name' is not defined"
     ]
    }
   ],
   "source": [
    "model_file = '%s-%s-%s.weights' % (env_name, alpha, episode)\n",
    "net.save_weights(model_file)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
